\chapter{Person Detection Development}
\label{chapter3}

\section{Deep Learning Approach}

To develop the person detection module two different approaches were presented in Chapter2. The HOG algorithm and deep-learning based models. Both methods make use of the incoming RGB data from the on-board RGB-D sensor.

The Histogram of Oriented Gradients (HOG) and its possible variances, such as the Oriented Histograms of Flow and Appearance (OHFA) and the AdaBoosted version, was the first natural choice for the task, due to its simplicity in the integration as the method was already implemented in the OpenCV library. However, the method's performance was acceptable only in close to perfect conditions due to the mechanics of the algorithm. Mainly when people are clearly visible in a seated or standing position, within sparse environments, and with no or little occlusions. False positives were also another problem encountered as shown in the pictures below. Moreover, the sliding window technique used by the method is rather inefficient as it has to search through the whole frame, making it computationally expensive for large ones.

On the other hand the deep-learning model proved to work really well under circumstances where HOG failed. Such as when people are standing or seated with partial body occlusion which is common in indoor environments. Moreover, each detection comes with the relative probability of the latter being a person. Allowing to discard detections with low confidence, something that was not possible with the HOG algorithm. Hence the decision to choose a deep leaning approach for the task.

\begin{figure}[H]
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.85\linewidth]{images/chapter3_hog.png}
        \caption{HOG detection with false positive.}
	\end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.85\linewidth]{images/chapter3_dnn.png}
        \caption{SSD detection with no false positives.}
	\end{subfigure}
    \label{fig:hog}
\end{figure}

\subsection{Stack}

The stack used for the development of the detection module includes the Python programming language and the OpenCV library.

The choice to use Python derives from the nature of the project. Being it an \textbf{Exploratory Software} product there was a lot of prototyping phases to it before settling for particular approaches or methodologies. Making of Python a natural choice for its dynamic behaviour and its ease of implementation. Moreover, both the ROS interfaces as well as the OpenCV library used offered Python bindings to call their relative APIs.

Version 3 of the Python programming language was used in particular along with OpenCV 2.4.8 and 3.3.0 to relatively access Indigo's \textbf{CvBridge} interface modules and the \textbf{dnn}\footnote{deep-neural-network} module to work with the serialised SSD network.

Two different versions of OpenCV were used because the ROS indigo version comes with standard static bindings for version 2.4.8 of the computer vision library. No easy install was also available to upgrade to the latest version of OpenCV, required for the dnn module, which would still be compatible with Indigo's CvBridge interface. A ROS distro upgrade wasn't an option either, given the need of the Indigo version for compatibility with TIAGO. The only remaining choice was to directly deal with a source installation and possibly fix the broken stating linkages along the way, which due to time constraints was avoided. Virtual environments were instead used to solve the issue.

\subsubsection{Virtual Environment Set-up}

A virtual environment is a cooperatively isolated runtime environment that allows Python users and applications to install and upgrade Python distribution packages without interfering with the behaviour of other Python applications running on the same system \cite{website:virtualEnv}.

To make use of such an environment some steps need to be taken. First of all the python package installer (pip) has to be installed if not already available and upgraded to the latest version. The last point is of particular importance as to access later on the latest version of OpenCV through pip, this has to be upgraded to the latest version as well.

The next step consists in installing the virtual-environment and the virtual-wrapper via pip which offers a higher level interface for the former to ease the creation of a personalised environment. However, before proceeding to the actual creation of the environment, the package installer cache needs to be cleaned as shown below, in order to avoid conflicting issues with older versions present within it.

\begin{lstlisting}[language=bash]
$ sudo rm -rf ~/.cache/pip
\end{lstlisting}

Next is the actual creation of the virtual-environment using the \textbf{mkvirtualenv} command made available through the virtual-wrapper package, that takes as a compulsory argument the name of the environment, in this case harn, and as an optional one the Python version to be installed within it. In this occasion Python3 was installed as it is a compulsory requirement for the OpenCV version to be installed. The creation command is as follows:

\begin{lstlisting}[language=bash]
$ mkvirtualenv harn -p Python3
\end{lstlisting}

Finally, the virtual-environment needs to be made accessible through simple bash commands, so as to activate and de-activate the environment easily. For this, the created virtual environment is exported in the \textbf{.bashrc} file which makes it available to all future bash sessions upon start. The export command is as follows:

\begin{lstlisting}[language=bash]
$ export WORKON_HOME=$HOME/.virtualenvs
\end{lstlisting}

\subsubsection{OpenCV 3.3.0 Installation}

Once the environment is set-up, activating and de-activating it is as easy as a matter of calling the \textbf{workon} and \textbf{deactivate} commands, also made available by the virtual-wrapper package. Upon activating the custom environment, the required OpenCV version can be installed using a simple pip install as shown below. The installation makes available the required dnn module.

\begin{lstlisting}[language=bash]
$ workon harn # This activates the environment called harn
(harn)$ pip install opencv-contrib-python # Installs OpenCV 3.3.0
(harn)$ deactivate # This de-activates the environment
$ # Now the environment is not active anymore
\end{lstlisting}

Whenever the detection module needs to be executed this takes place in the isolated and activated harn virtual-environment. Where the latest OpenCV and Python3 versions are made available. The remaining modules are run in normal environments.

\subsection{Module design}

The final solution design was affected by the Indigo-OpenCV compatibility issue presented before. More precisely, the detection module is not able to convert beforehand the RGB data incoming from the subscription into an OpenCV specific format, such as MAT, as the CvBridge interface is incompatible with the latest version of the computer vision library.

Hence, the decision to split the single detection module into two different ones. With one component performing the conversion and the other the detection. The conversion module is accountable for the conversion of the raw coloured image into a MAT format, using the standard OpenCV 2.4.8 version and the compatible CvBridge interface. The detection module, instead, is solely responsible to perform the detection process on the converted image.

\subsection{OpenCV conversion}

The conversion component is implemented as a collection of routines in the \textbf{cv\_conversion.py} file made available in project's GitHub repository. The steps to the conversion task are the following:

\begin{enumerate}
  \item Image topic subscription
  \item Callback conversion
  \item Conversion storage
\end{enumerate}

\subsubsection{Image Topic Subscription}

This step consists in subscribing to the raw stream of data published by the robot's RGB-D sensor. However, before being able to create the subscription a node has to be created. In fact, by making use of the ROS middleware, all communications happen over topics which allow the instances of the ROS graph, called nodes, to interact with each other by publishing or subscribing over the topic channels. 

To create the node the \textbf{rospy} library and the \textbf{init\_node}, whose arguments taken are two, are used. The parameters are the node's name so that it can be distinguished amongst the possible hundreds of nodes running in the graph, and a boolean flag, which if true, adds random numbers at the end of the node's name to ensure its uniqueness \cite{website:nodes}. The following is the node creation call:

\begin{lstlisting}
rospy.init_node('cv_conversion', anonymous=True)
\end{lstlisting}

Once the node is created, the subscription is a matter of using the \textbf{Subscriber} routine available through the \textbf{rospy} library. The Subscriber method takes three arguments. The first one is the topic string where the raw RGB data are being published by the robotics system in use, in this case \textbf{xtion/rgb/image\_raw}. The second argument is the message type being received through the subscription. ROS offers many built-in messages type, but for this particular subscription the data received is a standard\_msg/Image, which before being used has to be imported at the top of the script. The third and last argument is the callback function called once the data is received and whose task is to manipulate the raw information. The subscription call is as follows:

\begin{lstlisting}
rospy.Subscriber("/xtion/rgb/image_raw", Image, toMat)
\end{lstlisting}

\subsubsection{Callback Conversion}

The callback function has the task to manipulate the received data through the subscription. A callback function called \textbf{toMat} is defined which takes as argument the image\_raw data afterwards converted. Before describing the conversion step, it is interesting to note how the data is actually passed to the function. In fact, everything is handled by the Subscriber method which receives the data and does the call to the function reference passing the information received as an argument. Therefore no step in between is actually required.

The data conversion uses the \textbf{imgmsg\_to\_cv2} routine which is a method belonging to the CvBridge module. The function takes two arguments, the ROS image message to be converted and the encoding to convert it into. In this case, these are the image\_raw passed to the callback function and bgr8. The conversion call is as follows:

\begin{lstlisting}
cv_image = CvBridge().imgmsg_to_cv2(image_raw, 'bgr8')
\end{lstlisting}

\subsubsection{Conversion storage}

Lastly the converted image has to be made available to the detection module for the detection process. This is exactly why the conversion was stored in a variable instance named \textbf{cv\_image}, as this is passed as a parameter to a second function whose task is just to save the image to the disk. A function called \textbf{store} is defined, that takes as input the OpenCV image and calls the \textbf{imwrite} routine, available through OpenCV, to store the converted image in the specified path. The function call is as follows:

\begin{lstlisting}
cv2.imwrite(PATH + "/data/converted/image.png", cv_image)
\end{lstlisting}

where \textbf{PATH} is a constant variable storing a dynamic path to the folder used for the storage, so that no issues arise if the package is cloned in different paths by different users. The \textbf{path} python library is used to facilitate the path planning. The variable definition is as follows:

\begin{lstlisting}
PATH = str(Path(os.path.dirname(os.path.abspath(__file__))).parents[0])
\end{lstlisting}

\subsection{OpenCV detection}

The detection module, differently from the conversion component, is implemented as a class in the \textbf{human\_detection.py} file made available in the GitHub repository for the project. The steps to the detection process are the following:

\begin{enumerate}
  \item Model de-serialisation
  \item Image loading
  \item Feed-forward
  \item Process detections
  \item Message publication
\end{enumerate}

\subsubsection{Model De-Serialisation}

The original MobileNet version of the SSD model presented in Chapter1 was trained using Google's TensorFlow machine learning library by Howard et al. \cite{paper:MobileNets}. However, the version used for the project embedding makes use of the Caffe version trained by chuanqi305 \cite{website:chuanqi305}.

The trained model is able to detect over 20 different objects in images and is made available through serialised files that include the the \textbf{MobileNetSSD\_deploy.caffemodel} as well as the \textbf{MobileNetSSD\_deploy.prototxt}. The former defines the internal states and parameters for each layer, while the latter describes the architecture of the network, such as the input dimensions, the number of layers and the base network used as well as the activation function for every feature map.

Given that the neural network is serialised a de-serialisation process is required. The dnn module introduced in version 3.3.0 of OpenCV is used to load the serialised Caffe model. Caffe trained networks are not the only accepted ones, as the dnn library can work with a variety of other machine learning frameworks, including TensorFlow and Keras.

The de-serialisation or loading process does happen at the constructor level of the class that implements the detection module. In fact, by using an object-oriented design a single instance can be used to handle all detection requests afterwards. Therefore avoiding to load the model multiple times as only one de-serialisation is needed at the instantiation level. To perform the loading the \textbf{readFromCaffe} routine belonging to the dnn module's namespace is called, to which both the prototxt and .caffemodel files are passed as arguments as follows:

\begin{lstlisting}
self.net = cv2.dnn.readNetFromCaffe("prototxt.txt", "model.caffemodel")
\end{lstlisting}

\subsubsection{Image loading}

A further step is required before running the feed-forward routine made available through the de-serialisation of the model. This consists in actually loading the image to be fed to the neural network which was previously converted by the conversion module. OpenCV's \textbf{imread} routine is used for the task, whose only parameter is the path to the image. The function call is as follows:

\begin{lstlisting}
cv_image = cv2.imread(self.path + "/data/converted/image.png")
\end{lstlisting}

\subsubsection{Feed-forward}

The feed-forward process consists in feeding the loaded image to the neural network, which runs the detection algorithm to find people within the input image. However, before running the algorithm a couple more variables need to be initialised. These are the target variable, that identifies the class category of interest to be detected. In this case corresponding to the value 15 as people is the 15th category out of the 20 available. And the confidence variable which determines the minimum probability that a detection needs to have so as not to be filtered out. The confidence value of 0.5 turned out to be a good one. The array of targets containing all the possible categories is also defined.

Input-wise, the network accepts multiple image colour spaces, including grayscale and RGB with the only constraint that the input dimensions are of 300x300 pixels. This means that the original input has to undertake a pre-processing step to scale it down to the required size, as the original is of 640x480. Moreover, mean-subtraction and scaling are also applied to the resized image via the \textbf{blobFromImage} function to further improve the chances of obtaining a correct detection \cite{website:blobFromImage}. The input preparation logic is as follows:

\begin{lstlisting}
# Resize image
resized = cv2.resize(frame, (300, 300))
# Apply pre-processing steps
blob = cv2.dnn.blobFromImage(resized, 0.007843, (300, 300), 127.5)
\end{lstlisting}

The rest of the detection process is straightforward, as it is a matter of passing the pre-processed image to the network and run the feed-forward process. The dnn module offers routines for both of these tasks, by respectively calling the \textbf{setInput} method on the network instance and passing it the blob object as shown below:

\begin{lstlisting}
self.net.setInput(blob)
\end{lstlisting}

The detections result can then be obtained by running the \textbf{forwards} method on the same network instance as follows:

\begin{lstlisting}
detections = self.net.forwards()
\end{lstlisting}

\subsubsection{Process Detections}

The network is trained to recognise twenty different categories. This project, however, is only interested in detecting people presence in the image without much concern about any other object. Hence, within the iteration that loops over the detections returned by the feed-forward process, which is a multi-dimensional array, a conditional statement is added that checks whether the detection ID is equal to the previously defined target variable. The confidence variable, also previously defined, is also used within the same conditional statement to check that the person labelled detection is over a certain threshold by using an AND connective.

\begin{lstlisting}
# Loop over the detections
for i in np.arange(0, detections.shape[2]):
	# Get detection probability
	confidence = detections[0, 0, i, 2]

	# Get ID of the detection object
	idx = int(detections[0, 0, i, 1])

	# Filter out non-human detection with low confidence
	if confidence > self.confidence and idx == self.target:
    	# Rest of the logic goes in here
\end{lstlisting}

\textbf{Message publication}

Many are the information made available by the detections result tensor. The confidence and the categorical class for each detection are just two of these. The other important detail used in the project is the actual position of the person in the RGB frame, which for each detection is in terms of its top-left and bottom-right point. Hence, the width, height and consequently the centre point of the bounding-box around the detected person can be easily computed and published over a topic.

The publisher-subscriber is the main ROS communication paradigm, where a node can publish over a particular topic different messages and subscribe to as many others. Therefore, to allow interested users to have access to such information, the centre point of the detections obtained from the feed-forward process are made available over a topic called \textbf{detections}. This publishes a custom message defined for the project named \textbf{Detections}. The \textbf{Publisher} routine is used for this, which takes as the first argument the topic's name, followed by the message type and lastly the buffer size as follows:

\begin{lstlisting}
self.detection_pub = rospy.Publisher('detections', Detections, queue_size=5)
\end{lstlisting}

ROS already comes with standard data-types such as int, float and many others. Nonetheless, the creation of a new custom message to compactly store related data is straightforward. First a \textbf{msg} folder is created at the root level of the package, which will store the defined messages. Two messages are then created within the msg folder, one called \textbf{Detection.msg} which holds the details of the single detection, and another named \textbf{Detections.msg} that acts as a collection of all the details for all the detections that passed the if statement criteria. The two message definitions for the Detection.msg(left) and the Detections.msg(right) are shown below:

\begin{multicols}{2}
  \begin{itemize}
    \item int32 ID
    \item int32 centre\_x
    \item int32 centre\_y
  \end{itemize}

  \columnbreak

  \begin{itemize}
    \item Header header
    \item int32 number\_of\_detections
    \item Detection[] array
  \end{itemize}
\end{multicols}

The fields of the Detection message consist in three 32 bits integer fields. The ID one that will serve in later modules to correspond which distance or 3D position is to which detection, as well the centre\_x that stores the x-coordinate of the centre point of the detection and centre\_y for the y-coordinate. Publishing each detection message by itself would not make sense. Hence the need for the Detections message that has as fields a header storing information about the timestamp of the message, a 32 bits integer storing the number of the total detections for the input image and a collection of Detection messages. The custom messages before being available for usage need to be added to the CMAKE file at the root of the package and built using \textbf{catkin build}.

For each positive detection in the loop, a new Detection message instance is created and its fields are populated using Python's field access syntax. The ID takes the value of the current number\_of\_detections, which at first is 0 and is increased at every loop iteration, ultimately being set back to 0 at the end of the loop. The centre point of the detection's bounding-box is computed using the top-left and bottom-right information from the detections tensor for the specific instance, and its x and y coordinates are then stored in the relative fields. Every Detection object is then pushed to the Detections' array field using a simple append. The Detections message instance is a global one defined in the constructor, which gets re-instantiated at the end of each loop to clear it from the previous pushed content. The population is as follows:

\begin{lstlisting}
# Create Detection message
detection = Detection()

# Populate Detection message
detection.ID = self.number_of_detections
detection.centre_x = centre_ratio_point[0]
detection.centre_y = centre_ratio_point[1]

# Push Detection message to Detections array field
self.detections.array.append(detection)
\end{lstlisting}

At the end of each feed-forward request, the Detections message is published through the detections topic using the \textbf{publish} method on the publisher instance, which takes as its only argument the message to be published, in this case self.detections as follows:

\begin{lstlisting}
self.detection_pub.publish(self.detections)
\end{lstlisting}

An example output of the detections topic using the \textbf{topic echo} command is shown below:

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.5\linewidth]{images/chapter3.png}
  \end{center}
  \caption{Detections topic output.}
  \label{fig:det_topic}
\end{figure}

